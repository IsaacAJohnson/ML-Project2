metric = "ROC",
trControl = ctrl)
fit
confusionMatrix(predict(fit, test),factor(test$Churn))
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit)
print(fit$bestTune)
# the "method" below should match the one you chose above.
set.seed(1504) # I will choose a different seed for evaluation
bank_index <- createDataPartition(bank4$Churn, p = 0.80, list = FALSE)
train <- bank4[ bank_index, ]
test <- bank4[-bank_index, ]
# example spec for rf
fit_final <- train(Churn ~ .,
data = train,
method = "rf",
tuneGrid=fit$bestTune,
metric = "ROC",
trControl = ctrl)
# The last line means we will fit a model using the best tune parameters your CV found above.
myRoc <- roc(test$Churn, predict(fit_final, test, type="prob")[,2])
plot(myRoc)
auc(myRoc)
library(tidyverse)
library(caret)
library(pROC)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE)
bank = read_rds("../../BankChurners.rds")
library(tidyverse)
library(caret)
library(pROC)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE)
bank = read_rds("../BankChurners.rds")
# create some cool features. Make sure you add comments so I know what you are trying to accomplish!
#first try
library(forcats)
library(dplyr)
#EDA
glimpse(bank)
sum(is.na(bank))
dim(bank)
unique(bank$Education_Level)
unique(bank$Marital_Status)
unique(bank$Card_Category)
unique(bank$Income_Category)
bank %>%
group_by(Churn) %>%
count()
#engineering some features
#encoding categorical data
bank1 <- bank %>%
mutate(gender = if_else(Gender=="M",1,0)) %>%
select(-Gender) %>%
mutate(married = if_else(Marital_Status=="Married",1,0)) %>%
mutate(single = if_else(Marital_Status=="Single",1,0)) %>%
mutate(divorced = if_else(Marital_Status=="Divorced",1,0)) %>%
mutate(ms_unknown = if_else(Marital_Status=="Unknown",1,0)) %>%
select(-Marital_Status) %>%
mutate(blue = if_else(Card_Category=="Blue",1,0)) %>%
mutate(gold = if_else(Card_Category=="Gold",1,0)) %>%
mutate(silver = if_else(Card_Category=="Silver",1,0)) %>%
mutate(platinum = if_else(Card_Category=="Platinum",1,0)) %>%
select(-Card_Category) %>%
mutate(ed_level = Education_Level %>%
fct_recode("Unknown" = "Unknown",
"Uneducated" = "Uneducated",
"High School" = "High School",
"College" = "College",
"Graduate" = "Graduate",
"Post-Graduate" = "Post-Graduate",
"Doctorate" = "Doctorate") %>%
as.integer()) %>%
select(-Education_Level) %>%
mutate(income = Income_Category %>%
fct_recode("Unknown" = "Unknown",
"low" = "Less than $40K",
"lowmed" = "40K - $60K",
"med" = "$60K - $80K",
"medhigh" = "$80K - $120K",
"high" = "$120K +") %>%
as.integer()) %>%
select(-Income_Category)
bank2 <- bank1 %>%
mutate(churn = if_else(Churn=="yes",1,0)) %>%
select(-Churn)
round(cor(bank2), digits = 2)
cor(bank2$churn, bank2$Total_Trans_Ct)
cor(bank2$churn, bank2$Total_Ct_Chng_Q4_Q1)
cor(bank2$churn, bank2$Total_Revolving_Bal)
cor(bank2$churn, bank2$Total_Relationship_Count)
cor(bank2$churn, bank2$Contacts_Count_12_mon)
# bank3 <- bank1 %>%
#     select(Churn, Total_Trans_Ct, Total_Ct_Chng_Q4_Q1, Total_Revolving_Bal, Total_Relationship_Count, Contacts_Count_12_mon)
bank3 <- bank1 %>%
select(Credit_Limit, Customer_Age, Total_Revolving_Bal, Total_Trans_Ct,
Total_Trans_Amt, Churn)
print(ncol(bank3)) # Only 5 features allowed for project 2! Not counting the dependent variable.
# specify the model to be used (i.e. KNN, Naive Bayes, decision tree, random forest, bagged trees) and the tuning parameters used
ctrl <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(504)
bank_index <- createDataPartition(bank3$Churn, p = 0.80, list = FALSE)
train <- bank3[ bank_index, ]
test <- bank3[-bank_index, ]
# example spec for rf
fit <- train(Churn ~ .,
data = train,
method = "rf",
ntree = 20,
tuneLength = 3,
metric = "ROC",
trControl = ctrl)
fit
confusionMatrix(predict(fit, test),factor(test$Churn))
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit)
print(fit$bestTune)
# the "method" below should match the one you chose above.
set.seed(1504) # I will choose a different seed for evaluation
bank_index <- createDataPartition(bank3$Churn, p = 0.80, list = FALSE)
train <- bank3[ bank_index, ]
test <- bank3[-bank_index, ]
# example spec for rf
fit_final <- train(Churn ~ .,
data = train,
method = "rf",
tuneGrid=fit$bestTune,
metric = "ROC",
trControl = ctrl)
# The last line means we will fit a model using the best tune parameters your CV found above.
myRoc <- roc(test$Churn, predict(fit_final, test, type="prob")[,2])
plot(myRoc)
auc(myRoc)
library(tidyverse)
library(caret)
library(pROC)
library(MLmetrics)
knitr::opts_chunk$set(echo = TRUE)
#bank = read_rds("../../BankChurners.rds")
bank = read_rds("../BankChurners.rds")
# create some cool features. Make sure you add comments so I know what you are trying to accomplish!
#first try
library(forcats)
library(dplyr)
#EDA
glimpse(bank)
sum(is.na(bank))
dim(bank)
unique(bank$Education_Level)
unique(bank$Marital_Status)
unique(bank$Card_Category)
unique(bank$Income_Category)
bank %>%
group_by(Churn) %>%
count()
#engineering some features
#encoding categorical data
bank1 <- bank %>%
mutate(gender = if_else(Gender=="M",1,0)) %>%
select(-Gender) %>%
mutate(married = if_else(Marital_Status=="Married",1,0)) %>%
mutate(single = if_else(Marital_Status=="Single",1,0)) %>%
mutate(divorced = if_else(Marital_Status=="Divorced",1,0)) %>%
mutate(ms_unknown = if_else(Marital_Status=="Unknown",1,0)) %>%
select(-Marital_Status) %>%
mutate(blue = if_else(Card_Category=="Blue",1,0)) %>%
mutate(gold = if_else(Card_Category=="Gold",1,0)) %>%
mutate(silver = if_else(Card_Category=="Silver",1,0)) %>%
mutate(platinum = if_else(Card_Category=="Platinum",1,0)) %>%
select(-Card_Category) %>%
mutate(ed_level = Education_Level %>%
fct_recode("Unknown" = "Unknown",
"Uneducated" = "Uneducated",
"High School" = "High School",
"College" = "College",
"Graduate" = "Graduate",
"Post-Graduate" = "Post-Graduate",
"Doctorate" = "Doctorate") %>%
as.integer()) %>%
select(-Education_Level) %>%
mutate(income = Income_Category %>%
fct_recode("Unknown" = "Unknown",
"low" = "Less than $40K",
"lowmed" = "40K - $60K",
"med" = "$60K - $80K",
"medhigh" = "$80K - $120K",
"high" = "$120K +") %>%
as.integer()) %>%
select(-Income_Category)
bank2 <- bank1 %>%
mutate(churn = if_else(Churn=="yes",1,0)) %>%
select(-Churn)
round(cor(bank2), digits = 2)
cor(bank2$churn, bank2$Total_Trans_Ct)
cor(bank2$churn, bank2$Total_Ct_Chng_Q4_Q1)
cor(bank2$churn, bank2$Total_Revolving_Bal)
cor(bank2$churn, bank2$Total_Relationship_Count)
cor(bank2$churn, bank2$Contacts_Count_12_mon)
# bank3 <- bank1 %>%
#     select(Churn, Total_Trans_Ct, Total_Ct_Chng_Q4_Q1, Total_Revolving_Bal, Total_Relationship_Count, Contacts_Count_12_mon)
bank3 <- bank1 %>%
select(Credit_Limit, Customer_Age, Total_Revolving_Bal, Total_Trans_Ct,
Total_Trans_Amt, Churn)
print(ncol(bank3)) # Only 5 features allowed for project 2! Not counting the dependent variable.
# specify the model to be used (i.e. KNN, Naive Bayes, decision tree, random forest, bagged trees) and the tuning parameters used
ctrl <- trainControl(method = "cv", number = 3, classProbs = TRUE, summaryFunction = twoClassSummary)
set.seed(504)
bank_index <- createDataPartition(bank3$Churn, p = 0.80, list = FALSE)
train <- bank3[ bank_index, ]
test <- bank3[-bank_index, ]
# example spec for rf
fit <- train(Churn ~ .,
data = train,
method = "rf",
ntree = 20,
tuneLength = 3,
metric = "ROC",
trControl = ctrl)
fit
confusionMatrix(predict(fit, test),factor(test$Churn))
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit)
print(fit$bestTune)
# the "method" below should match the one you chose above.
set.seed(1504) # I will choose a different seed for evaluation
bank_index <- createDataPartition(bank3$Churn, p = 0.80, list = FALSE)
train <- bank3[ bank_index, ]
test <- bank3[-bank_index, ]
# example spec for rf
fit_final <- train(Churn ~ .,
data = train,
method = "rf",
tuneGrid=fit$bestTune,
metric = "ROC",
trControl = ctrl)
# The last line means we will fit a model using the best tune parameters your CV found above.
myRoc <- roc(test$Churn, predict(fit_final, test, type="prob")[,2])
plot(myRoc)
auc(myRoc)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
data(stop_words)
wine <- read_rds("../variety.rds") %>% rowid_to_column("id")
glimpse(wine)
top_words <- wine %>%
unnest_tokens(word, description) %>%
anti_join(stop_words) %>%
filter(!(word %in% c("wine","flavors","pinot","gris"))) %>%
count(id, word) %>%
group_by(id) %>%
mutate(exists = if_else(n>0,1,0)) %>%
ungroup %>%
right_join(wine, by="id") %>%
count(variety, word) %>%
group_by(variety) %>%
top_n(3,n) %>%
ungroup %>%
select(word) %>%
distinct()
top_words
wino <- wine %>%
unnest_tokens(word, description) %>%
anti_join(stop_words) %>%
filter(word %in% top_words$word) %>%
count(id, word) %>%
group_by(id) %>%
mutate(exists = if_else(n>0,1,0)) %>%
ungroup %>%
pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = c(exists=0)) %>%
right_join(wine, by="id") %>%
replace(.,is.na(.),0) %>%
mutate(price=log(price)) %>%
mutate(price=scale(price), points=scale(points)) %>%
select(-id,-variety, -description)
head(wino)
kclust <- kmeans(wino, centers = 3)
kclust$centers
glance(kclust)
View(wine)
wink <- augment(kclust,wino)
head(wink)
wink %>%
pivot_longer(c(oak, finish, fruit),names_to = "feature") %>%
ggplot(aes(value, fill=.cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
wink %>%
pivot_longer(c(cherry, pear, apple),names_to = "feature") %>%
ggplot(aes(value, fill=.cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
wink %>%
pivot_longer(c(points,price),names_to = "feature") %>%
ggplot(aes(value, fill=.cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
kclusts <- tibble(k = 1:9) %>%
mutate(
kclust = map(k, ~kmeans(wino, .x)),
glanced = map(kclust, glance),
augmented = map(kclust, augment, wino)
)
assignments <- kclusts %>%
unnest(augmented)
ggplot(assignments, aes(price, points)) +
geom_point(aes(color = .cluster), alpha=0.3) +
facet_wrap(~ k)
clusterings <- kclusts %>%
unnest(glanced, .drop = TRUE)
ggplot(clusterings, aes(k, tot.withinss)) +
geom_line()
bank <- read_rds("/Users/isaacjohnson/Documents/Scanner Output/School/Willamette/Machine Learning/Week 10/BankChurners.rds")
View(bank)
kclustbank <- kmeans(bank, centers = 3)
#setting # of clusters to 3 (that's K)
kclustbank <- kmeans(bank, centers = 10)
#setting # of clusters to 3 (that's K)
kclustbank <- kmeans(bank, centers = 5)
bank <- bank %>%
select(1:12)
library(fastDummies)
bank <- bank %>%
select(1:12) %>%
dummy_cols()
bank <- bank %>%
select(1:12) %>%
dummy_cols() %>%
select(-Gender, -Education_Level, -Marital_Status, -Income_Category, -Card_Category)
kclustbank <- kmeans(bank, centers = 5)
kclustbank$centers
glance(kclustbank)
bankK <- augment(kclust,bank)
bankK <- augment(kclustbank,bank)
head(bankK)
kclustbank <- kmeans(bank, centers = 3)
kclustbank$centers
glance(kclustbank)
bankK <- augment(kclustbank,bank)
head(bankK)
bankK %>%
pivot_longer(c(Customer_Age, Dependent_count, Months_on_book),names_to = "feature") %>%
ggplot(aes(value, fill=.cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
bankK <- augment(kclustbank,bank)
head(bankK)
kclustbank$centers
bankK %>%
pivot_longer(c(Customer_Age, Dependent_count, Months_on_book),names_to = "feature") %>%
ggplot(aes(value, fill=.cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
bankK <- augment(kclustbank,bank)
head(bankK)
View(bankK)
View(bankK)
kclustbank$centers
swine <- wino %>%
sample_n(200)
hclustr <- hclust(d=dist(swine))
summary(hclustr)
plot(hclustr)
abline(h=3, col="red")
hclustr <- hclust(d=dist(wino))
cluster <- cutree(hclustr, k=3)
swine <- wino %>%
add_column(cluster) %>%
mutate(cluster=as_factor(cluster))
head(swine)
swine %>%
pivot_longer(c(oak, finish, fruit),names_to = "feature") %>%
ggplot(aes(value, fill=cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
swine %>%
pivot_longer(c(cherry, pear, apple),names_to = "feature") %>%
ggplot(aes(value, fill=cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
swine %>%
pivot_longer(c(points,price),names_to = "feature") %>%
ggplot(aes(value, fill=cluster))+
geom_density(alpha=0.3)+
facet_wrap(~feature)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(caret)
library(fastDummies)
library(randomForest)
# https://www.openml.org/d/1590
raw_income = read_csv("./openml_1590.csv", na=c("?"))
income = read_csv("./openml_1590.csv", na=c("?")) %>%
drop_na() %>%
mutate(income_above_50k = class==">50K") %>%
select(-class) %>%
dummy_cols(remove_selected_columns = T)
View(income)
View(raw_income)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(dbscan)
data(stop_words)
wine <- read_rds("../resources/variety.rds") %>% rowid_to_column("id")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidytext)
library(dbscan)
data(stop_words)
wine <- read_rds("../variety.rds") %>% rowid_to_column("id")
glimpse(wine)
wine <- wine %>%
unnest_tokens(word, description) %>%
anti_join(stop_words) %>%
filter(!(word %in% c("drink","vineyard","variety","price","points","wine","pinot","chardonnay","gris","noir","riesling","syrah"))) %>%
count(id, word) %>%
group_by(word) %>%
mutate(total = sum(n)) %>%
filter(total > 100) %>%
ungroup %>%
group_by(id) %>%
mutate(exists = if_else(n>0,1,0)) %>%
ungroup %>%
pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = c(exists=0)) %>%
right_join(wine, by="id") %>%
replace(.,is.na(.),0) %>%
mutate(log_price = log(price)) %>%
select(-id, -price, -description, -variety)
names(wine)
lof <- lof(wine, minPts = 10)
summary(lof)
hist(lof, breaks = 10, main = "LOF (minPts = 10)")
plot(sort(lof), type = "l",  main = "LOF (minPts = 10)",
xlab = "Points sorted by LOF", ylab = "LOF")
plot(select(wine, c("points", "log_price")), pch = ".", main = "LOF (minPts = 10)", asp = 1)
points(select(wine, c("points", "log_price")), cex = (lof - 1) * 4, pch = 1, col = "red")
text(wine[lof > 1.3,], labels = round(lof, 1)[lof > 1.3], pos = 3)
combined = bind_cols(as.data.frame(lof, names=c("LOF")), read_rds("../resources/variety.rds") %>% rowid_to_column("id") )
combined = bind_cols(as.data.frame(lof, names=c("LOF")), read_rds("../variety.rds") %>% rowid_to_column("id") )
View(select(combined, lof, description, variety, price, points))
bank = read_rds("../BankChurners.rds")
bank = read_rds("/Users/isaacjohnson/Documents/Scanner Output/School/Willamette/Machine Learning/Week 10/BankChurners.rds")
library(isotree)
model = isolation.forest(bank, ndim=1, ntrees=10)
scores = predict(model, bank, type="score")
hist(scores, breaks = 10, main = "IF Scores")
plot(sort(scores), type = "l",  main = "IF Scores",
xlab = "Points sorted by score", ylab = "IF score")
View(bank)
glance(scores)
plot(select(wine, c("points", "log_price")), pch = ".", main = "IF Scores", asp = 1)
points(select(wine, c("points", "log_price"))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red")
plot(select(bank, select1:5), pch = ".", main = "IF Scores", asp = 1)
View(bank)
plot(select(bank, c("Total_Relationship_Count", "Credit_Limit")), pch = ".", main = "IF Scores", asp = 1)
points(select(bank, c("Total_Relationship_Count", "Credit_Limit"))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red")
plot(select(bank, c("Total_Relationship_Count", "Credit_Limit")), pch = ".", main = "IF Scores", asp = 1, xlim=c(1,6))
points(select(bank, c("Total_Relationship_Count", "Credit_Limit"))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red")
plot(select(bank, c("Total_Relationship_Count", "Credit_Limit")), pch = ".", main = "IF Scores", asp = 1)
points(select(bank, c("Total_Relationship_Count", "Credit_Limit"))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red")
plot(select(bank, c(12, 9)), pch = ".", main = "IF Scores", asp = 1)
points(select(bank, c(12, 9))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red")
plot(select(bank, c(9, 16)), pch = ".", main = "IF Scores", asp = 1)
points(select(bank, c(9, 16))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red")
plot(select(bank, c(9, 12)), pch = ".", main = "IF Scores", asp = 1)
points(select(bank, c(9, 12))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red", xlim=c(1,6))
plot(select(bank, c(9, 12)), pch = ".", main = "IF Scores", asp = 1, xlim=c(1,6))
points(select(bank, c(9, 12))[scores > 0.5,], cex = as.data.frame(scores)[scores > 0.5,] * 4, pch = 1, col = "red", xlim=c(1,6))
library(tidyverse)
data()
View(starwars)
test <- startwars %>%
filter(species=="Human") %>%
mutate(bmi=mass/(height^2)) %>%
group_by(sex) %>%
summary()
bmi <- startwars %>%
filter(species=="Human" %>%)
bmi <- startwars %>%
filter(species=="Human") %>%
mutate(bmi=mass/(height^2))
bmi <- starwars %>%
filter(species=="Human") %>%
mutate(bmi=mass/(height^2))
bmi
View(bmi)
bmi %>% group_by(sex)
bmi %>% group_by(sex) %>%
summary()
bmi %>%
group_by(sex) %>%
avg(bmi)
bmi %>%
group_by(sex) %>%
mean(bmi)
skimr::skim(bmi)
bmi %>%
mean(bmi$bmi)
drop_na(bmi)
bmi %>%
group_by(bmi) %>%
summarise(meanBmi=mean(bmi))
bmi %>%
group_by(sex) %>%
summarise(meanBmi=mean(bmi))
bmi %>%
drop_na() %>%
group_by(sex) %>%
summarise(meanBmi=mean(bmi))
